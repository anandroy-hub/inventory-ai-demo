import streamlit as st
import pandas as pd
import numpy as np
import re
import os
from difflib import SequenceMatcher

# Advanced AI/ML Imports
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
import plotly.express as px
import plotly.graph_objects as go

# --- PAGE CONFIG ---
st.set_page_config(page_title="AI Inventory Auditor Pro", layout="wide", page_icon="­ЪЏА№ИЈ")

# --- KNOWLEDGE BASE: DOMAIN LOGIC ---
DEFAULT_PRODUCT_GROUP = "Consumables & General"

PRODUCT_GROUPS = {
┬а ┬а "Piping & Fittings": ["FLANGE", "PIPE", "ELBOW", "TEE", "UNION", "REDUCER", "BEND", "COUPLING", "NIPPLE", "BUSHING", "UPVC", "CPVC", "PVC"],
┬а ┬а "Valves & Actuators": ["BALL VALVE", "GATE VALVE", "PLUG VALVE", "CHECK VALVE", "GLOBE VALVE", "CONTROL VALVE", "VALVE", "ACTUATOR", "COCK"],
┬а ┬а "Fasteners & Seals": ["STUD", "BOLT", "NUT", "WASHER", "GASKET", "O RING", "MECHANICAL SEAL", "SEAL", "JOINT"],
┬а ┬а "Electrical & Instruments": ["TRANSMITTER", "CABLE", "WIRE", "GAUGE", "SENSOR", "CONNECTOR", "SWITCH", "TERMINAL", "INSTRUMENT", "CAMERA"],
┬а ┬а "Tools & Hardware": ["PLIER", "CUTTING PLIER", "STRIPPER", "WIRE STRIPPER", "WRENCH", "SPANNER", "HAMMER", "FILE", "SAW", "TOOL", "CHISEL", "CUTTER", "TAPE MEASURE", "MEASURING TAPE", "BIT", "DRILL BIT"],
┬а ┬а "Consumables & General": ["BRUSH", "PAINT BRUSH", "TAPE", "ADHESIVE", "HOSE", "SAFETY GLOVE", "GLOVE", "CLEANER", "PAINT", "CEMENT", "STICKER", "CHALK"],
┬а ┬а "Specialized Spares": ["FILTER", "BEARING", "PUMP", "MOTOR", "CARTRIDGE", "IMPELLER", "SPARE"]
}

SPEC_TRAPS = {
┬а ┬а "Gender": ["MALE", "FEMALE"],
┬а ┬а "Connection": ["BW", "SW", "THD", "THREADED", "FLGD", "FLANGED", "SORF", "WNRF", "BLRF"],
┬а ┬а "Rating": ["150#", "300#", "600#", "PN10", "PN16", "PN25", "PN40"]
}

# --- AI UTILITIES ---
def clean_description(text):
┬а ┬а text = str(text).upper().replace('"', ' ')
┬а ┬а text = text.replace("O-RING", "O RING")
┬а ┬а text = text.replace("MECH-SEAL", "MECHANICAL SEAL").replace("MECH SEAL", "MECHANICAL SEAL")
┬а ┬а text = re.sub(r'[^A-Z0-9\s./-]', ' ', text)
┬а ┬а return re.sub(r'\s+', ' ', text).strip()

def token_pattern(token):
┬а ┬а return rf'(?<!\w){re.escape(token)}(?!\w)'

def get_tech_dna(text):
┬а ┬а text = clean_description(text)
┬а ┬а dna = {"numbers": set(re.findall(r'\d+(?:[./]\d+)?', text)), "attributes": {}}
┬а ┬а for cat, keywords in SPEC_TRAPS.items():
┬а ┬а ┬а ┬а found = [k for k in keywords if re.search(token_pattern(k), text)]
┬а ┬а ┬а ┬а if found: dna["attributes"][cat] = set(found)
┬а ┬а return dna

def intelligent_noun_extractor(text):
┬а ┬а text = clean_description(text)
┬а ┬а phrases = ["MEASURING TAPE", "BALL VALVE", "GATE VALVE", "PLUG VALVE", "CHECK VALVE", "MECHANICAL SEAL", "PAINT BRUSH", "WIRE STRIPPER", "CUTTING PLIER", "DRILL BIT"]
┬а ┬а for p in phrases:
┬а ┬а ┬а ┬а if re.search(token_pattern(p), text): return p
┬а ┬а all_nouns = [item for sublist in PRODUCT_GROUPS.values() for item in sublist]
┬а ┬а for n in all_nouns:
┬а ┬а ┬а ┬а if re.search(token_pattern(n), text): return n
┬а ┬а return text.split()[0] if text.split() else "MISC"

def map_product_group(noun):
┬а ┬а for group, keywords in PRODUCT_GROUPS.items():
┬а ┬а ┬а ┬а if noun in keywords:
┬а ┬а ┬а ┬а ┬а ┬а return group
┬а ┬а for group, keywords in PRODUCT_GROUPS.items():
┬а ┬а ┬а ┬а for keyword in keywords:
┬а ┬а ┬а ┬а ┬а ┬а if re.search(token_pattern(keyword), noun):
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а return group
┬а ┬а return DEFAULT_PRODUCT_GROUP

def dominant_group(series):
┬а ┬а counts = series.value_counts()
┬а ┬а return counts.idxmax() if not counts.empty else "UNMAPPED"

# --- MAIN ENGINE ---
@st.cache_data
def run_intelligent_audit(file_path):
┬а ┬а df = pd.read_csv(file_path, encoding='latin1')
┬а ┬а df.columns = [c.strip() for c in df.columns]
┬а ┬а id_col = next(c for c in df.columns if any(x in c.lower() for x in ['item', 'no']))
┬а ┬а desc_col = next(c for c in df.columns if 'desc' in c.lower())
┬а ┬а┬а
┬а ┬а df['Standard_Desc'] = df[desc_col].apply(clean_description)
┬а ┬а df['Part_Noun'] = df['Standard_Desc'].apply(intelligent_noun_extractor)
┬а ┬а df['Product_Group'] = df['Part_Noun'].apply(map_product_group)

┬а ┬а # NLP & Topic Modeling
┬а ┬а tfidf = TfidfVectorizer(max_features=300, stop_words='english')
┬а ┬а tfidf_matrix = tfidf.fit_transform(df['Standard_Desc'])
┬а ┬а┬а
┬а ┬а # Clustering for Confidence
┬а ┬а kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)
┬а ┬а df['Cluster_ID'] = kmeans.fit_predict(tfidf_matrix)
┬а ┬а dists = kmeans.transform(tfidf_matrix)
┬а ┬а df['Confidence'] = (1 - (np.min(dists, axis=1) / np.max(dists, axis=1))).round(4)
┬а ┬а cluster_groups = df.groupby('Cluster_ID')['Product_Group'].agg(dominant_group)
┬а ┬а df['Cluster_Group'] = df['Cluster_ID'].map(cluster_groups)
┬а ┬а df['Cluster_Validated'] = df['Product_Group'] == df['Cluster_Group']
┬а ┬а┬а
┬а ┬а # Anomaly
┬а ┬а iso = IsolationForest(contamination=0.04, random_state=42)
┬а ┬а df['Anomaly_Flag'] = iso.fit_predict(tfidf_matrix) # Using tfidf for complexity-based anomalies

┬а ┬а # Fuzzy & Tech DNA
┬а ┬а df['Tech_DNA'] = df['Standard_Desc'].apply(get_tech_dna)
┬а ┬а┬а
┬а ┬а return df, id_col, desc_col

# --- DATA LOADING ---
target_file = 'raw_data.csv'
if os.path.exists(target_file):
┬а ┬а df_raw, id_col, desc_col = run_intelligent_audit(target_file)
else:
┬а ┬а st.error("Data file missing from repository. Please ensure 'raw_data.csv' is present.")
┬а ┬а st.stop()

# Filter defaults
group_options = list(PRODUCT_GROUPS.keys())

# --- HEADER & MODERN NAVIGATION ---
st.title("­ЪЏА№ИЈ AI Inventory Auditor Pro")
st.markdown("### Advanced Inventory Intelligence & Quality Management")

# Modern horizontal tab navigation
page = st.tabs(["­ЪЊѕ Executive Dashboard", "­ЪЊЇ Categorization Audit", "­Ъџе Quality Hub (Anomalies/Dups)", "­ЪДа Technical Methodology"])

# Initialize filtered dataframe (will be filtered per page as needed)
df = df_raw.copy()

# --- PAGE: EXECUTIVE DASHBOARD ---
with page[0]:
┬а ┬а st.markdown("#### ­ЪЊі Inventory Health Overview")
┬а ┬а st.markdown("Get a bird's eye view of your inventory data quality and distribution.")
┬а ┬а┬а
┬а ┬а # Filters at the top
┬а ┬а with st.container():
┬а ┬а ┬а ┬а st.markdown("##### ­ЪћЇ Filters")
┬а ┬а ┬а ┬а selected_group = st.multiselect("Product Category", options=group_options, default=group_options, key="dash_group")
┬а ┬а┬а
┬а ┬а # Apply Filters
┬а ┬а df = df_raw[df_raw['Product_Group'].isin(selected_group)]
┬а ┬а┬а
┬а ┬а st.markdown("---")
┬а ┬а┬а
┬а ┬а # KPI Row
┬а ┬а kpi1, kpi2, kpi3, kpi4 = st.columns(4)
┬а ┬а kpi1.metric("­ЪЊд SKUs Analyzed", len(df))
┬а ┬а kpi2.metric("­Ъј» Mean AI Confidence", f"{df['Confidence'].mean():.1%}")
┬а ┬а kpi3.metric("Рџа№ИЈ Anomalies Found", len(df[df['Anomaly_Flag'] == -1]))
┬а ┬а kpi4.metric("­Ъћё Duplicate Pairs", "Audit Required")

┬а ┬а st.markdown("---")
┬а ┬а┬а
┬а ┬а col1, col2 = st.columns(2)
┬а ┬а with col1:
┬а ┬а ┬а ┬а fig_pie = px.pie(df, names='Product_Group', title="Inventory Distribution by Product Category", hole=0.4)
┬а ┬а ┬а ┬а st.plotly_chart(fig_pie, use_container_width=True)
┬а ┬а with col2:
┬а ┬а ┬а ┬а top_nouns = df['Part_Noun'].value_counts().head(10).reset_index()
┬а ┬а ┬а ┬а fig_bar = px.bar(top_nouns, x='Part_Noun', y='count', title="Top 10 Product Categories", labels={'Part_Noun':'Product', 'count':'Qty'})
┬а ┬а ┬а ┬а st.plotly_chart(fig_bar, use_container_width=True)

┬а ┬а # Health Gauge
┬а ┬а health_val = (len(df[df['Anomaly_Flag'] == 1]) / len(df)) * 100
┬а ┬а fig_gauge = go.Figure(go.Indicator(
┬а ┬а ┬а ┬а mode = "gauge+number",
┬а ┬а ┬а ┬а value = health_val,
┬а ┬а ┬а ┬а title = {'text': "Catalog Data Accuracy %"},
┬а ┬а ┬а ┬а gauge = {'axis': {'range': [0, 100]}, 'bar': {'color': "#00cc96"}}
┬а ┬а ))
┬а ┬а st.plotly_chart(fig_gauge, use_container_width=True)

# --- PAGE: CATEGORIZATION AUDIT ---
with page[1]:
┬а ┬а st.markdown("#### ­ЪЊЇ AI Categorization & Filtered Audit")
┬а ┬а st.markdown("Drill down into specific product categories with intelligent filtering.")
┬а ┬а┬а
┬а ┬а # Filters at the top of the table
┬а ┬а with st.container():
┬а ┬а ┬а ┬а st.markdown("##### ­ЪћЇ Filters")
┬а ┬а ┬а ┬а selected_group = st.multiselect("Product Category", options=group_options, default=group_options, key="cat_group")
┬а ┬а┬а
┬а ┬а # Apply Filters
┬а ┬а df = df_raw[df_raw['Product_Group'].isin(selected_group)]
┬а ┬а┬а
┬а ┬а st.markdown("---")
┬а ┬а st.markdown(f"**Showing {len(df)} items**")
┬а ┬а┬а
┬а ┬а # Data Table with sorting
┬а ┬а st.dataframe(
┬а ┬а ┬а ┬а df[[id_col, 'Standard_Desc', 'Part_Noun', 'Product_Group', 'Confidence']].sort_values('Confidence', ascending=False),
┬а ┬а ┬а ┬а use_container_width=True,
┬а ┬а ┬а ┬а height=400
┬а ┬а )
┬а ┬а┬а
┬а ┬а summary = (
┬а ┬а ┬а ┬а df.groupby('Product_Group', dropna=False)
┬а ┬а ┬а ┬а .agg(Items=(id_col, 'count'), Mean_Confidence=('Confidence', 'mean'), Cluster_Match_Rate=('Cluster_Validated', 'mean'))
┬а ┬а ┬а ┬а .reset_index()
┬а ┬а ┬а ┬а .sort_values('Items', ascending=False)
┬а ┬а )
┬а ┬а summary['Mean_Confidence'] = summary['Mean_Confidence'].round(3)
┬а ┬а summary['Cluster_Match_Rate'] = summary['Cluster_Match_Rate'].round(3)
┬а ┬а st.markdown("#### ­ЪЊї Category Distribution & Confidence")
┬а ┬а st.dataframe(summary, use_container_width=True, height=260)

┬а ┬а # Distribution of confidence
┬а ┬а fig_hist = px.histogram(df, x="Confidence", nbins=20, title="Confidence Score Distribution", color_discrete_sequence=['#636EFA'])
┬а ┬а st.plotly_chart(fig_hist, use_container_width=True)

# --- PAGE: QUALITY HUB ---
with page[2]:
┬а ┬а st.markdown("#### ­Ъџе Anomaly & Duplicate Identification")
┬а ┬а st.markdown("Identify quality issues and potential duplicates in your inventory data.")
┬а ┬а┬а
┬а ┬а # Filters at the top
┬а ┬а with st.container():
┬а ┬а ┬а ┬а st.markdown("##### ­ЪћЇ Filters")
┬а ┬а ┬а ┬а selected_group = st.multiselect("Product Category", options=group_options, default=group_options, key="qual_group")
┬а ┬а┬а
┬а ┬а # Apply Filters
┬а ┬а df = df_raw[df_raw['Product_Group'].isin(selected_group)]
┬а ┬а┬а
┬а ┬а st.markdown("---")
┬а ┬а┬а
┬а ┬а t1, t2 = st.tabs(["Рџа№ИЈ Anomalies", "­ЪЉ» Fuzzy Duplicates"])
┬а ┬а┬а
┬а ┬а with t1:
┬а ┬а ┬а ┬а st.subheader("Statistical Anomalies (Isolation Forest)")
┬а ┬а ┬а ┬а anoms = df[df['Anomaly_Flag'] == -1]
┬а ┬а ┬а ┬а st.warning(f"Found {len(anoms)} anomalies in the current view.")
┬а ┬а ┬а ┬а st.dataframe(anoms[[id_col, desc_col, 'Part_Noun']], use_container_width=True, height=400)
┬а ┬а ┬а ┬а┬а
┬а ┬а with t2:
┬а ┬а ┬а ┬а st.subheader("Fuzzy Duplicate Audit (Spec-Aware)")
┬а ┬а ┬а ┬а st.info("System identifies items with >85% text similarity but differentiates based on numeric specs (Size/Gender).")
┬а ┬а ┬а ┬а┬а
┬а ┬а ┬а ┬а # Calculate fuzzy duplicates for the current view
┬а ┬а ┬а ┬а fuzzy_list = []
┬а ┬а ┬а ┬а recs = df.to_dict('records')
┬а ┬а ┬а ┬а for i in range(len(recs)):
┬а ┬а ┬а ┬а ┬а ┬а for j in range(i + 1, min(i + 50, len(recs))): # Smaller window for real-time speed
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а r1, r2 = recs[i], recs[j]
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а sim = SequenceMatcher(None, r1['Standard_Desc'], r2['Standard_Desc']).ratio()
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а if sim > 0.85:
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а dna1, dna2 = r1['Tech_DNA'], r2['Tech_DNA']
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а is_variant = (dna1['numbers'] != dna2['numbers']) or (dna1['attributes'] != dna2['attributes'])
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а fuzzy_list.append({
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а 'ID A': r1[id_col], 'ID B': r2[id_col],
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а 'Desc A': r1['Standard_Desc'], 'Desc B': r2['Standard_Desc'],
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а 'Match %': f"{sim:.1%}", 'Verdict': "­ЪЏа№ИЈ Variant" if is_variant else "­Ъџе Duplicate"
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а })
┬а ┬а ┬а ┬а┬а
┬а ┬а ┬а ┬а if fuzzy_list:
┬а ┬а ┬а ┬а ┬а ┬а st.dataframe(pd.DataFrame(fuzzy_list), use_container_width=True, height=400)
┬а ┬а ┬а ┬а else:
┬а ┬а ┬а ┬а ┬а ┬а st.success("No fuzzy duplicates found in this filtered view.")

# --- PAGE: METHODOLOGY ---
with page[3]:
┬а ┬а st.markdown("#### ­ЪДа Technical Methodology & AI Stack")
┬а ┬а st.markdown("Understand the advanced algorithms powering this inventory intelligence system.")
┬а ┬а┬а
┬а ┬а st.markdown("""
┬а ┬а ### 1. Data Processing (ETL)
┬а ┬а We standardize the raw 543 rows by stripping quote artifacts, uppercasing, and cleaning symbols. We utilize **RegEx** to extract technical specifications (Numbers, Sizes, Genders) into a "Technical DNA" profile for every part.
┬а ┬а┬а
┬а ┬а ### 2. Intelligent Categorization
┬а ┬а Instead of standard K-Means (which is biased by word frequency), we use a **Prioritized Knowledge Base** to anchor nouns to super-categories. This ensures that tools like 'Pliers' aren't mislabeled as 'Pipes' just because they both mention a 'Size'.
┬а ┬а┬а
┬а ┬а ### 3. Cluster Validation
┬а ┬а We validate the knowledge-anchored categories against **K-Means** clusters to ensure semantic consistency before scoring confidence.
┬а ┬а┬а
┬а ┬а ### 4. Anomaly Detection
┬а ┬а We use the **Isolation Forest** algorithm. It isolates observations by randomly selecting a feature and then randomly selecting a split value. Outliers (anomalies) are easier to isolate, resulting in shorter paths.
┬а ┬а┬а
┬а ┬а ### 5. Fuzzy Match & Conflict Resolution
┬а ┬а We use the **Levenshtein Distance** algorithm. However, we've added a **Business Logic Layer**: if two items have similar text but conflicting 'Technical DNA' (e.g. one is Male, one is Female), the system overrides the AI and flags it as a **Variant**, not a duplicate.
┬а ┬а """)
